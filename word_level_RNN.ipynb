{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing all the necessary libraries"
      ],
      "metadata": {
        "id": "mA4EjrppmR1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "ZY-_xYmPkFBy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have worked on google colab to make use of GPUs for parallel processing. Below, I have mounted google drive to read the dataset and to save the model. "
      ],
      "metadata": {
        "id": "KcoLG56smaRW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1HjMAsvE7ga0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f457f4-6559-4f4d-975f-565bc2e118fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset: Alice's Adventures in Wonderland (commonly Alice in Wonderland)\n",
        "\n",
        "It is an 1865 English novel by Lewis Carroll. It details the story of a young girl named Alice who falls through a rabbit hole into a fantasy world of anthropomorphic creatures."
      ],
      "metadata": {
        "id": "bA4TJSLCnLH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to read the text file \n",
        "with open('/content/gdrive/MyDrive/text_generation/alice_in_wonderland.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "text = text[:1000000] \n",
        "print(f'first 100 characters:\\n {text[:100]}')"
      ],
      "metadata": {
        "id": "ufMxKlb88XMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f8d4007-bb80-4583-874a-adaeebd8d75c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 100 characters:\n",
            " Alice's Adventures in Wonderland\n",
            "\n",
            "                ALICE'S ADVENTURES IN WONDERLAND\n",
            "\n",
            "                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "siZzdvKe8-w6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computers only understand numbers so we need to convert words to integers. Words are mapped to integers based on the number of times a word has occured in the corpus. "
      ],
      "metadata": {
        "id": "RbWEER3Aq3iD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mapping(text: str): \n",
        "  '''\n",
        "  Function description: \n",
        "      A function to create word tokens, word to integer mapping, and integer to word mapping.\n",
        "\n",
        "  parameters: \n",
        "      text: a variable containing the document.\n",
        "\n",
        "  Returns: \n",
        "      word_tokens: tokenization of corpus\n",
        "      word2int: word to integer mapping\n",
        "      int2word: integer to word mapping \n",
        "  '''\n",
        "\n",
        "  word_tokens = [word.text for word in nlp(text)]\n",
        "  sorted_unique_words = sorted(Counter(word_tokens).items(), key = lambda x:x[1], reverse = True) # vocabulary \n",
        "  word2int = {word:id for id, (word, count) in enumerate(sorted_unique_words)} \n",
        "  int2word = {id:word for id, (word, count) in enumerate(sorted_unique_words)} \n",
        "\n",
        "  return word_tokens, word2int, int2word\n"
      ],
      "metadata": {
        "id": "D0MHgmy0p6FS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens, word2int, int2word = mapping(text)"
      ],
      "metadata": {
        "id": "FFxF9lCo9L9o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'number of words in the corpus are: {len(word_tokens)}\\nnumber of words in the vocubulary or unique words in the corpus are: {len(word2int)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S427Qc6buKce",
        "outputId": "a6274071-d19d-47b4-b437-9581f671a387"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of words in the corpus are: 37958\n",
            "number of words in the vocubulary or unique words in the corpus are: 3139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the whole corpus to int values\n",
        "intarr = [word2int[i] for i in word_tokens]\n",
        "print(f'first 100 word tokens converted to integer values: \\n {intarr[:100]}')"
      ],
      "metadata": {
        "id": "nTDwY-UMAJeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff5cbbb1-ef76-4951-b62d-09e4b74e28dd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first 100 word tokens converted to integer values: \n",
            " [17, 29, 919, 19, 920, 758, 921, 515, 1649, 1188, 1650, 1651, 1652, 1653, 1654, 426, 1655, 1656, 1657, 1658, 1659, 339, 11, 922, 923, 2, 119, 36, 1660, 288, 17, 18, 289, 8, 120, 40, 516, 14, 388, 94, 23, 427, 1, 30, 2, 924, 0, 6, 14, 389, 164, 8, 44, 24, 9, 158, 68, 759, 13, 31, 1, 925, 76, 2, 362, 23, 427, 18, 926, 0, 37, 12, 31, 73, 1, 760, 68, 1661, 19, 12, 0, 4, 6, 56, 51, 2, 243, 14, 10, 362, 0, 3, 1, 69, 17, 4, 189, 760, 68, 390]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size: int, seq_len: int):\n",
        "  ''' Function description: \n",
        "        A function to create input and output batches of dimension (batch_size, seq_len)\n",
        "\n",
        "      Parameters: \n",
        "      arr: integer converted word tokens\n",
        "      seq_len: number of words in a sequence in a batch \n",
        "      batch_size: number of sequences in a batch\n",
        "\n",
        "      Returns:\n",
        "      x: input batch\n",
        "      y: corresponding output batch     \n",
        "  '''\n",
        "  arr = np.asarray(arr)\n",
        "  n_batches = int(len(arr)//(batch_size * seq_len))\n",
        "  arr = arr[: n_batches * batch_size * seq_len]\n",
        "  arr = arr.reshape((batch_size, -1))\n",
        "\n",
        "  for n in range(0, arr.shape[1], seq_len):\n",
        "    x = arr[:, n: n + seq_len]    \n",
        "    y = np.zeros_like(x)\n",
        "    \n",
        "    try:\n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n + seq_len]\n",
        "    except IndexError: \n",
        "      y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "    yield x, y \n"
      ],
      "metadata": {
        "id": "UQZ4Tw5MBH27"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, seq_len = 64, 32\n",
        "x, y = next(get_batches(intarr, batch_size, seq_len))\n",
        "print(f'each input batch dimension {x.shape}')\n",
        "print(f'each output batch dimension {y.shape}')\n"
      ],
      "metadata": {
        "id": "Tz84qeL6AN9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803ef4b3-22bd-4dc5-ffd8-295eb6e84c4c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "each input batch dimension (64, 32)\n",
            "each output batch dimension (64, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "WK6MF-FTEWk4"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent Neural Networks can be used for language modeling. I have selected one of its variant, LSTM (long-short term memory) because the model can remember the information of long term words. In cell state of the LSTM, the neccesary information about previous words will be remembered and unneccessary information about previous words will be removed. Based on this, the hidden state will be updated."
      ],
      "metadata": {
        "id": "cVa8DYnB2lXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class wordRNN(nn.Module):\n",
        "  def __init__(self, intarr, embedding_dim = 50, n_hidden = 512, n_layers = 2, drop_prob = 0.5):\n",
        "    super().__init__()\n",
        "    self.n_hidden = n_hidden\n",
        "    self.n_layers = n_layers\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.n_words = len(set(intarr)) # number of unique words in the corpus \n",
        "    self.embedding = nn.Embedding(self.n_words, self.embedding_dim)\n",
        "    self.lstm = nn.LSTM(self.embedding_dim, self.n_hidden, self.n_layers, batch_first = True)\n",
        "    self.dropout = nn.Dropout(drop_prob)\n",
        "    self.linear = nn.Linear(self.n_hidden, self.n_words)\n",
        "  \n",
        "  def forward(self, x, hidden):\n",
        "    # x = [batch_size, seq_len] --> (64, 32)\n",
        "\n",
        "    embed = self.embedding(x)\n",
        "    # embed = [batch_size, seq_len, embedding_dim] --> (64, 32, 50)\n",
        "\n",
        "    r_output, hidden = self.lstm(embed, hidden)\n",
        "    # r_output = [batch_size, seq_len, hidden_size] --> (64, 32, 512)\n",
        "    # hidden = tuple(hn, cn) where hn = [n_layers, batch_size, hidden_size] --> (2, 64, 512)\n",
        "\n",
        "    out = self.dropout(r_output)\n",
        "    out = out.contiguous().view(-1, self.n_hidden)\n",
        "    # out: (-1, 512) --> [batch_size * seq_len, hidden_size] --> (2048, 512)\n",
        "\n",
        "    out = self.linear(out)\n",
        "    # out = [batch_size * seq_len, n_words] --> (2048, 3139)\n",
        "    \n",
        "    return out, hidden\n",
        "\n",
        "  def init_hidden(self, batch_size):\n",
        "    hidden = (torch.zeros(self.n_layers, batch_size, self.n_hidden).to(device),\n",
        "              torch.zeros(self.n_layers, batch_size, self.n_hidden).to(device))\n",
        "    return hidden "
      ],
      "metadata": {
        "id": "fYCsRDzsI-PY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = wordRNN(intarr)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "OX4yoH_9PJdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc50f573-afd1-4ffb-bbfe-b4ab99fa2c5f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "wordRNN(\n",
              "  (embedding): Embedding(3139, 50)\n",
              "  (lstm): LSTM(50, 512, num_layers=2, batch_first=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (linear): Linear(in_features=512, out_features=3139, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "leSLomgCSXVu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "80% of batches are selected for training and 20% of batches are selected for validation. "
      ],
      "metadata": {
        "id": "aAjrs_tb3qwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_batches = int(len(intarr)/(batch_size * seq_len))\n",
        "n_training_batches = int(np.floor(0.8 * n_batches))\n",
        "n_validation_batches = int(np.ceil(0.2 * n_batches))\n",
        "print(f'number of batches: {n_batches}\\nnumber of training batches: {n_training_batches}\\nnumber of validation batches: {n_validation_batches}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6M64zxtg3m13",
        "outputId": "c4276cda-fd0e-47d0-9bd2-d2596dd10746"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of batches: 18\n",
            "number of training batches: 14\n",
            "number of validation batches: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a word from dataset, we will train the model to predict next word in a sequence. \n",
        "\n",
        "We will check the perplexity (evaluation metrics for language model) of the model during validation. Lower the perplexity, better the model. And then saving the model which gives lower validation loss (low perplexity). "
      ],
      "metadata": {
        "id": "zk-muSGg4qwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, seq_len = 64, 32\n",
        "epochs = 50\n",
        "min_validation_loss = np.inf\n",
        "\n",
        "\n",
        "for i in range(epochs):\n",
        "  print(f'epoch {i}')\n",
        "  validation_loss = 0\n",
        "  hidden = model.init_hidden(batch_size)\n",
        "\n",
        "  for batch, (words, labels) in enumerate(get_batches(intarr, batch_size, seq_len)):\n",
        "    words = torch.tensor(words).to(device)\n",
        "    labels = torch.tensor(labels).to(device)\n",
        "    labels = labels.view(batch_size * seq_len).long()\n",
        "\n",
        "    if batch <= n_training_batches - 1: \n",
        "      model.train()\n",
        "      hidden = tuple([i.data for i in hidden])\n",
        "      optimizer.zero_grad()\n",
        "      logits, hidden = model(words, hidden)\n",
        "      loss = criterion(logits, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      if batch % 100 == 0:\n",
        "        print(f'training_loss: {loss : .3f}')\n",
        "\n",
        "    if batch == n_training_batches:\n",
        "      hidden = model.init_hidden(batch_size)\n",
        "\n",
        "    if batch > n_training_batches - 1:\n",
        "      model.eval()\n",
        "      logits, hidden = model(words, hidden)\n",
        "      loss = criterion(logits, labels)\n",
        "\n",
        "      validation_loss += loss.item()\n",
        "\n",
        "\n",
        "  average_validation_loss = validation_loss/ n_validation_batches    \n",
        "  print(f'average_validation_loss: {average_validation_loss: .3f}, perplexity: {torch.exp(torch.tensor(average_validation_loss))}')\n",
        "\n",
        "  if min_validation_loss > average_validation_loss:\n",
        "    print(f'Validation Loss Decreased({min_validation_loss:.6f}--->{average_validation_loss:.6f}) \\t Saving The Model')\n",
        "    min_validation_loss = average_validation_loss\n",
        "    torch.save(model.state_dict(), '/content/gdrive/MyDrive/text_generation/model.pt')\n"
      ],
      "metadata": {
        "id": "bwVpHLJaJTcg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2ee7c6f-b682-4293-c813-4fe43d6173bb"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0\n",
            "training_loss:  8.053\n",
            "average_validation_loss:  5.995, perplexity: 401.4327087402344\n",
            "Validation Loss Decreased(inf--->5.995040) \t Saving The Model\n",
            "epoch 1\n",
            "training_loss:  6.173\n",
            "average_validation_loss:  5.914, perplexity: 370.0849304199219\n",
            "Validation Loss Decreased(5.995040--->5.913732) \t Saving The Model\n",
            "epoch 2\n",
            "training_loss:  6.023\n",
            "average_validation_loss:  5.909, perplexity: 368.2083435058594\n",
            "Validation Loss Decreased(5.913732--->5.908649) \t Saving The Model\n",
            "epoch 3\n",
            "training_loss:  5.975\n",
            "average_validation_loss:  5.908, perplexity: 368.1451416015625\n",
            "Validation Loss Decreased(5.908649--->5.908477) \t Saving The Model\n",
            "epoch 4\n",
            "training_loss:  5.927\n",
            "average_validation_loss:  5.899, perplexity: 364.6735534667969\n",
            "Validation Loss Decreased(5.908477--->5.899003) \t Saving The Model\n",
            "epoch 5\n",
            "training_loss:  5.874\n",
            "average_validation_loss:  5.857, perplexity: 349.6988525390625\n",
            "Validation Loss Decreased(5.899003--->5.857072) \t Saving The Model\n",
            "epoch 6\n",
            "training_loss:  5.824\n",
            "average_validation_loss:  5.769, perplexity: 320.30059814453125\n",
            "Validation Loss Decreased(5.857072--->5.769260) \t Saving The Model\n",
            "epoch 7\n",
            "training_loss:  5.722\n",
            "average_validation_loss:  5.647, perplexity: 283.5013122558594\n",
            "Validation Loss Decreased(5.769260--->5.647217) \t Saving The Model\n",
            "epoch 8\n",
            "training_loss:  5.594\n",
            "average_validation_loss:  5.556, perplexity: 258.79901123046875\n",
            "Validation Loss Decreased(5.647217--->5.556052) \t Saving The Model\n",
            "epoch 9\n",
            "training_loss:  5.496\n",
            "average_validation_loss:  5.486, perplexity: 241.24423217773438\n",
            "Validation Loss Decreased(5.556052--->5.485810) \t Saving The Model\n",
            "epoch 10\n",
            "training_loss:  5.402\n",
            "average_validation_loss:  5.428, perplexity: 227.66615295410156\n",
            "Validation Loss Decreased(5.485810--->5.427880) \t Saving The Model\n",
            "epoch 11\n",
            "training_loss:  5.324\n",
            "average_validation_loss:  5.383, perplexity: 217.7190399169922\n",
            "Validation Loss Decreased(5.427880--->5.383206) \t Saving The Model\n",
            "epoch 12\n",
            "training_loss:  5.247\n",
            "average_validation_loss:  5.337, perplexity: 207.91604614257812\n",
            "Validation Loss Decreased(5.383206--->5.337134) \t Saving The Model\n",
            "epoch 13\n",
            "training_loss:  5.180\n",
            "average_validation_loss:  5.300, perplexity: 200.2534637451172\n",
            "Validation Loss Decreased(5.337134--->5.299584) \t Saving The Model\n",
            "epoch 14\n",
            "training_loss:  5.101\n",
            "average_validation_loss:  5.262, perplexity: 192.7904510498047\n",
            "Validation Loss Decreased(5.299584--->5.261604) \t Saving The Model\n",
            "epoch 15\n",
            "training_loss:  5.038\n",
            "average_validation_loss:  5.229, perplexity: 186.63584899902344\n",
            "Validation Loss Decreased(5.261604--->5.229159) \t Saving The Model\n",
            "epoch 16\n",
            "training_loss:  4.967\n",
            "average_validation_loss:  5.191, perplexity: 179.5648651123047\n",
            "Validation Loss Decreased(5.229159--->5.190536) \t Saving The Model\n",
            "epoch 17\n",
            "training_loss:  4.895\n",
            "average_validation_loss:  5.147, perplexity: 171.97750854492188\n",
            "Validation Loss Decreased(5.190536--->5.147364) \t Saving The Model\n",
            "epoch 18\n",
            "training_loss:  4.829\n",
            "average_validation_loss:  5.112, perplexity: 166.00851440429688\n",
            "Validation Loss Decreased(5.147364--->5.112039) \t Saving The Model\n",
            "epoch 19\n",
            "training_loss:  4.755\n",
            "average_validation_loss:  5.084, perplexity: 161.34019470214844\n",
            "Validation Loss Decreased(5.112039--->5.083515) \t Saving The Model\n",
            "epoch 20\n",
            "training_loss:  4.703\n",
            "average_validation_loss:  5.054, perplexity: 156.585205078125\n",
            "Validation Loss Decreased(5.083515--->5.053601) \t Saving The Model\n",
            "epoch 21\n",
            "training_loss:  4.639\n",
            "average_validation_loss:  5.019, perplexity: 151.29539489746094\n",
            "Validation Loss Decreased(5.053601--->5.019234) \t Saving The Model\n",
            "epoch 22\n",
            "training_loss:  4.591\n",
            "average_validation_loss:  4.987, perplexity: 146.4459228515625\n",
            "Validation Loss Decreased(5.019234--->4.986656) \t Saving The Model\n",
            "epoch 23\n",
            "training_loss:  4.537\n",
            "average_validation_loss:  4.965, perplexity: 143.3659210205078\n",
            "Validation Loss Decreased(4.986656--->4.965400) \t Saving The Model\n",
            "epoch 24\n",
            "training_loss:  4.455\n",
            "average_validation_loss:  4.939, perplexity: 139.56407165527344\n",
            "Validation Loss Decreased(4.965400--->4.938524) \t Saving The Model\n",
            "epoch 25\n",
            "training_loss:  4.401\n",
            "average_validation_loss:  4.921, perplexity: 137.13995361328125\n",
            "Validation Loss Decreased(4.938524--->4.921002) \t Saving The Model\n",
            "epoch 26\n",
            "training_loss:  4.367\n",
            "average_validation_loss:  4.923, perplexity: 137.34622192382812\n",
            "epoch 27\n",
            "training_loss:  4.332\n",
            "average_validation_loss:  4.895, perplexity: 133.64007568359375\n",
            "Validation Loss Decreased(4.921002--->4.895150) \t Saving The Model\n",
            "epoch 28\n",
            "training_loss:  4.285\n",
            "average_validation_loss:  4.898, perplexity: 134.06369018554688\n",
            "epoch 29\n",
            "training_loss:  4.251\n",
            "average_validation_loss:  4.868, perplexity: 130.1002960205078\n",
            "Validation Loss Decreased(4.895150--->4.868306) \t Saving The Model\n",
            "epoch 30\n",
            "training_loss:  4.196\n",
            "average_validation_loss:  4.865, perplexity: 129.6731414794922\n",
            "Validation Loss Decreased(4.868306--->4.865017) \t Saving The Model\n",
            "epoch 31\n",
            "training_loss:  4.152\n",
            "average_validation_loss:  4.849, perplexity: 127.56616973876953\n",
            "Validation Loss Decreased(4.865017--->4.848635) \t Saving The Model\n",
            "epoch 32\n",
            "training_loss:  4.096\n",
            "average_validation_loss:  4.848, perplexity: 127.51107025146484\n",
            "Validation Loss Decreased(4.848635--->4.848203) \t Saving The Model\n",
            "epoch 33\n",
            "training_loss:  4.052\n",
            "average_validation_loss:  4.850, perplexity: 127.71187591552734\n",
            "epoch 34\n",
            "training_loss:  4.017\n",
            "average_validation_loss:  4.832, perplexity: 125.43689727783203\n",
            "Validation Loss Decreased(4.848203--->4.831803) \t Saving The Model\n",
            "epoch 35\n",
            "training_loss:  3.991\n",
            "average_validation_loss:  4.827, perplexity: 124.81876373291016\n",
            "Validation Loss Decreased(4.831803--->4.826863) \t Saving The Model\n",
            "epoch 36\n",
            "training_loss:  3.924\n",
            "average_validation_loss:  4.819, perplexity: 123.83035278320312\n",
            "Validation Loss Decreased(4.826863--->4.818912) \t Saving The Model\n",
            "epoch 37\n",
            "training_loss:  3.859\n",
            "average_validation_loss:  4.801, perplexity: 121.6508560180664\n",
            "Validation Loss Decreased(4.818912--->4.801155) \t Saving The Model\n",
            "epoch 38\n",
            "training_loss:  3.837\n",
            "average_validation_loss:  4.796, perplexity: 121.01034545898438\n",
            "Validation Loss Decreased(4.801155--->4.795876) \t Saving The Model\n",
            "epoch 39\n",
            "training_loss:  3.780\n",
            "average_validation_loss:  4.782, perplexity: 119.32101440429688\n",
            "Validation Loss Decreased(4.795876--->4.781817) \t Saving The Model\n",
            "epoch 40\n",
            "training_loss:  3.741\n",
            "average_validation_loss:  4.775, perplexity: 118.53813171386719\n",
            "Validation Loss Decreased(4.781817--->4.775235) \t Saving The Model\n",
            "epoch 41\n",
            "training_loss:  3.690\n",
            "average_validation_loss:  4.773, perplexity: 118.28523254394531\n",
            "Validation Loss Decreased(4.775235--->4.773099) \t Saving The Model\n",
            "epoch 42\n",
            "training_loss:  3.668\n",
            "average_validation_loss:  4.800, perplexity: 121.51681518554688\n",
            "epoch 43\n",
            "training_loss:  3.647\n",
            "average_validation_loss:  4.774, perplexity: 118.36383056640625\n",
            "epoch 44\n",
            "training_loss:  3.571\n",
            "average_validation_loss:  4.800, perplexity: 121.4690170288086\n",
            "epoch 45\n",
            "training_loss:  3.555\n",
            "average_validation_loss:  4.787, perplexity: 119.97277069091797\n",
            "epoch 46\n",
            "training_loss:  3.502\n",
            "average_validation_loss:  4.791, perplexity: 120.3830795288086\n",
            "epoch 47\n",
            "training_loss:  3.455\n",
            "average_validation_loss:  4.785, perplexity: 119.66607666015625\n",
            "epoch 48\n",
            "training_loss:  3.415\n",
            "average_validation_loss:  4.800, perplexity: 121.4623031616211\n",
            "epoch 49\n",
            "training_loss:  3.360\n",
            "average_validation_loss:  4.806, perplexity: 122.26869201660156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model which has best perplexity score \n",
        "model.load_state_dict(torch.load('/content/gdrive/MyDrive/text_generation/model.pt'))"
      ],
      "metadata": {
        "id": "J1lrQyyZiyAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc414cfe-10d1-4e93-f1d6-86c8ad343c5e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will see how our model predicts with greedy search algorithm (selecting the highest probability from all the probabilities that the model predicted for every unique words in the corpus)"
      ],
      "metadata": {
        "id": "IZLaEqVo6NHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(n_words, word):\n",
        "  ''' '''\n",
        "\n",
        "  word = word2int[word]\n",
        "  model.eval()\n",
        "  hidden = model.init_hidden(1)\n",
        "  predicted_words = []\n",
        "  predicted_words.append(word)\n",
        "  \n",
        "  for i in range(n_words):\n",
        "    wordint = torch.tensor(word)\n",
        "    wordint = wordint.view(-1, 1).to(device)\n",
        "\n",
        "    logits, hidden = model(wordint, hidden)\n",
        "    prob = F.softmax(logits, dim = 1).data.cpu()\n",
        "    word = np.argmax(prob).item()\n",
        "\n",
        "    predicted_words.append(word)\n",
        "\n",
        "  return ' '.join([int2word[i] for i in predicted_words])\n",
        "\n",
        "\n",
        "predicted_words = predict(100, 'The')"
      ],
      "metadata": {
        "id": "ELMXaAhkEGFk"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "iDEuPLlT7WU0",
        "outputId": "96e98bf2-baf8-4c77-a2bf-c09a4c398340"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The \\n thing , ' said the King , ` and \\n I 'm a little thing , ' said the King , ` and \\n I 'm a little thing , ' said the King , ` and \\n I 'm a little thing , ' said the King , ` and \\n I 'm a little thing , ' said the King , ` and \\n I 'm a little thing , ' said the King , ` and \\n I 'm a little thing , ' said the King , ` and \\n I 'm a little thing\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will see how our model predicts by randomly selecting a probability from top k higher probabilities. "
      ],
      "metadata": {
        "id": "6REVnGNy6gtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_top_k(n_words, sentence):\n",
        "\n",
        "  words_int = [word2int[i.text] for i in nlp(sentence)]\n",
        "\n",
        "  model.eval()\n",
        "  hidden = model.init_hidden(1)\n",
        "  predicted_words = []\n",
        "\n",
        "  for i in words_int:\n",
        "    wordint = torch.tensor(i)\n",
        "    wordint = wordint.view(-1, 1).to(device)\n",
        "    logits, hidden = model(wordint, hidden)\n",
        "    prob = F.softmax(logits, dim = 1).data.cpu()\n",
        "\n",
        "    prob, top_words= prob.topk(10)\n",
        "    prob, top_words= prob.numpy().squeeze(), top_words.numpy().squeeze()\n",
        "    word = np.random.choice(top_words)\n",
        "\n",
        "  predicted_words.append(word)\n",
        "\n",
        "  for i in range(n_words):\n",
        "    wordint = torch.tensor(word)\n",
        "    wordint = wordint.view(-1, 1).to(device)\n",
        "\n",
        "    logits, hidden = model(wordint, hidden)\n",
        "    prob = F.softmax(logits, dim = 1).data.cpu()\n",
        "\n",
        "    prob, top_words = prob.topk(10)\n",
        "    prob, top_words = prob.numpy().squeeze(), top_words.numpy().squeeze()\n",
        "    word = np.random.choice(top_words)\n",
        "\n",
        "    predicted_words.append(word)\n",
        "\n",
        "  return ' '.join([int2word[i] for i in (words_int + predicted_words)])     \n"
      ],
      "metadata": {
        "id": "54PTB2_AxUZ0"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_words = predict_top_k(100, 'I wonder if')"
      ],
      "metadata": {
        "id": "ucFWPe5m_btt"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "ja_N_hSw8qc8",
        "outputId": "2cf81b04-b476-4de6-c8f8-e8c2613fa1b4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I wonder if I ca n\\'t get that what \\n it was a \\n way in be to say the whole - box -- but if it might n\\'t \\n have seen a \\n deal , you dear to the right - key of them of the same , \\n           And I can do n\\'t have the day \\n to think the same I - day ? \\n       Oh . \\n The thing - head \\'s well off ; \" \" You are sure , and all -- I think that you know to \\n all , you \\'ll do all , \\' and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}